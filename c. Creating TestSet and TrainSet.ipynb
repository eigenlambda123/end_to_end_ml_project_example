{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a11b908-ba8a-40f4-b6c4-e19cce157b8e",
   "metadata": {},
   "source": [
    "### Creating TestSet and TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676e82f6-7a8c-4daa-9ce6-50005f6ec338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")  # Path to the compressed dataset\n",
    "\n",
    "    if not tarball_path.is_file():  # If the file doesn't exist locally\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)  # Create the 'datasets' directory if needed\n",
    "\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"  # URL to download the dataset\n",
    "        urllib.request.urlretrieve(url, tarball_path)  # Download the .tgz file from the URL and save it locally\n",
    "\n",
    "        with tarfile.open(tarball_path) as housing_tarball:  # Open the .tgz file as a tar archive\n",
    "            housing_tarball.extractall(path=\"datasets\")  # Extract all contents into the 'datasets' directory\n",
    "\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))  # Load the CSV data into a DataFrame and return it\n",
    "\n",
    "housing = load_housing_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96c7d5-cb5d-4177-87e1-69cae57340f9",
   "metadata": {},
   "source": [
    "### Naive Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7807f9f0-54a1-4d19-9fe0-0591b4eb0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data)) # Shuffled data\n",
    "    test_set_size = int(len(data) * test_ratio) # length of  data * 0.2(since we get 20% for test set)\n",
    "    test_indices = shuffled_indices[:test_set_size] # 20% testset\n",
    "    train_indices = shuffled_indices[test_set_size:] # 80% trainset\n",
    "    return data.iloc[train_indices], data.iloc[test_indices] # return seperate test and trainset data\n",
    "\n",
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852416f5-f7f4-4a5b-8549-4ca1b39070f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd51380-780f-4d77-b052-3e2fd53fef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3eaadd-2149-41a9-b2d8-66765d2574ae",
   "metadata": {},
   "source": [
    "###  Stable or Deterministic Split via Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529458a1-a3a1-4b42-a379-b2f6ad4b1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32  # Import the crc32 hash function from the zlib module (used for stable hashing)\n",
    "import numpy as np      # Needed for converting to int64\n",
    "\n",
    "# This function determines whether a given ID should go into the test set\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    # Convert the identifier to a 64-bit integer (ensures consistent input type for hashing)\n",
    "    # Apply crc32 hashing to generate a 32-bit deterministic hash value\n",
    "    # Return True if the hash value is within the test_ratio proportion of the total possible range (0 to 2^32)\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32 \n",
    "\n",
    "# This function splits a dataset into a stable train/test split based on hashed IDs\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    # Extract the column containing unique IDs\n",
    "    ids = data[id_column]\n",
    "\n",
    "    # Apply the is_id_in_test_set function to each ID\n",
    "    # This returns a boolean Series: True if the row should go to the test set\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "\n",
    "    # Use the boolean Series to filter and return:\n",
    "    # - rows not in test set (i.e., training set)\n",
    "    # - rows in test set\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "493a40cf-638a-4804-8285-bbaf2cbac529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'id' column to uniquely identify each row based on location, \n",
    "# since the dataset does not come with a built-in unique identifier\n",
    "# We combine 'longitude' and 'latitude' into a single number by multiplying 'longitude' to preserve decimal precision and reduce collisions.\n",
    "# This works because location is a stable feature â€” even if we update the dataset with new entries, \n",
    "# existing rows will retain their 'id' and remain consistently assigned to the same split.\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "\n",
    "# Use the deterministic hashing method to split the dataset into training and test sets based on the 'id' column.\n",
    "# About 20% of the rows will go into the test set if their hashed 'id' falls below the threshold, \n",
    "# ensuring a stable and consistent train/test split across dataset updates.\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ccd3b7-8996-43b5-8dc3-e98c36e7af1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -122192.12\n",
       "1       -122182.14\n",
       "2       -122202.15\n",
       "3       -122212.15\n",
       "4       -122212.15\n",
       "           ...    \n",
       "20635   -121050.52\n",
       "20636   -121170.51\n",
       "20637   -121180.57\n",
       "20638   -121280.57\n",
       "20639   -121200.63\n",
       "Name: id, Length: 20640, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_with_id[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a51401e-dd49-49c2-a777-8ef21aecea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16322"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3947c283-35b5-427c-884b-b7d74b94c4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4318"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

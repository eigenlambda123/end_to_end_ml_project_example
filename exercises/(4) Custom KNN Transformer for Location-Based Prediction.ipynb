{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d3b3d8",
   "metadata": {},
   "source": [
    "# Housing Model Training Pipeline + Hyperparameter Tuners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libraries",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "Import all necessary libraries for data processing, transformation, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "library-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-transformer",
   "metadata": {},
   "source": [
    "## 2. Define Custom Transformer\n",
    "The `ClusterSimilarity` transformer creates geographic cluster features using KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cluster-similarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates features based on similarity to geographic clusters\n",
    "    \n",
    "    Parameters:\n",
    "    n_clusters : Number of clusters to create\n",
    "    gamma : Controls the influence radius of clusters\n",
    "    random_state : Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        if hasattr(X, \"values\"):\n",
    "            X = X.values\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, \n",
    "                             random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if hasattr(X, \"values\"):\n",
    "            X = X.values\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.kmeans_.cluster_centers_, \n",
    "                                 axis=2)\n",
    "        return np.exp(-self.gamma * distances ** 2)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\"n_clusters\": self.n_clusters, \n",
    "                \"gamma\": self.gamma, \n",
    "                \"random_state\": self.random_state}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data\n",
    "Load the housing dataset and create stratified train/test splits based on income categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-housing-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "    \"\"\"Load California housing dataset from remote URL\"\"\"\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "# Load and prepare data\n",
    "housing = load_housing_data()\n",
    "\n",
    "# Create income categories for stratified sampling\n",
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "# Split into stratified train/test sets\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing,\n",
    "    test_size=0.2,\n",
    "    stratify=housing[\"income_cat\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "housing = strat_train_set.drop([\"median_house_value\", \"income_cat\"], axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-definition",
   "metadata": {},
   "source": [
    "## 4. Define Transformation Pipelines\n",
    "Create custom transformation pipelines for different feature types including ratio features, log transformations, and categorical encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feature-pipelines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for ratio features\n",
    "def column_ratio(X):\n",
    "    \"\"\"Calculate ratio between two columns\"\"\"\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    \"\"\"Generate feature name for ratio features\"\"\"\n",
    "    return [\"ratio\"]\n",
    "\n",
    "# Pipeline for ratio features\n",
    "def ratio_pipeline():\n",
    "    \"\"\"Pipeline for ratio features: Impute → Calculate ratio → Scale\"\"\"\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler()\n",
    "    )\n",
    "\n",
    "# Pipeline for log-transformed features\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "# Default pipeline for numeric features\n",
    "default_num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# Initialize geographic similarity transformer\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply-preprocessing",
   "metadata": {},
   "source": [
    "## 6. Apply Preprocessing\n",
    "Transform the training data and verify the output feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "transform-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data shape: (16512, 24)\n",
      "\n",
      "Expected features breakdown:\n",
      "  - 3 ratio features\n",
      "  - 5 log-transformed features\n",
      "  - 10 geographic similarity features\n",
      "  - 5 one-hot encoded categories\n",
      "  - 1 remaining numeric feature\n",
      "Total: 24 features\n"
     ]
    }
   ],
   "source": [
    "# Apply the full preprocessing pipeline\n",
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "\n",
    "# Display resulting feature matrix shape\n",
    "print(\"Preprocessed data shape:\", housing_prepared.shape)\n",
    "print(\"\\nExpected features breakdown:\")\n",
    "print(\"  - 3 ratio features\")\n",
    "print(\"  - 5 log-transformed features\")\n",
    "print(\"  - 10 geographic similarity features\")\n",
    "print(\"  - 5 one-hot encoded categories\")\n",
    "print(\"  - 1 remaining numeric feature\")\n",
    "print(\"Total: 24 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237eee5-b7f3-48eb-b5e8-52981dcb6d36",
   "metadata": {},
   "source": [
    "---\n",
    "# **Exercise**\n",
    "* Try adding a SelectFromModel transformer in the preparation pipeline\n",
    "to select only the most important attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2fa69-e1ba-41f8-8f9d-faa8d0d692f4",
   "metadata": {},
   "source": [
    "### **Define the Custom Transformer**\n",
    "**Create `KNNPriceFeature`**\n",
    "* This transformer learns the price patterns of nearby locations using `KNeighborsRegressor`.\n",
    "In `fit()`, it trains on latitude/longitude and `median_house_value`.  \n",
    "In `transform()`, it outputs the predicted price for any (lat, long) location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b436048-b82b-4f79-a0fe-93e8c6be6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class KNNPriceFeature(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer that predicts housing prices using KNeighborsRegressor\n",
    "    based on latitude and longitude. Outputs prediction as a new feature.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors=5, weights='distance'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, \"values\"):\n",
    "            X = X.values\n",
    "        self.knn_ = KNeighborsRegressor(n_neighbors=self.n_neighbors, weights=self.weights)\n",
    "        self.knn_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if hasattr(X, \"values\"):\n",
    "            X = X.values\n",
    "        return self.knn_.predict(X).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c2f29-80a0-4ea4-8aa0-fb98a5ebc889",
   "metadata": {},
   "source": [
    "### **Add It to the Preprocessing Pipeline**\n",
    "**Add to Pipeline**\n",
    "* We include the `KNNPriceFeature` as one of the transformation steps.  \n",
    "It uses only the `latitude` and `longitude` columns, and returns one new feature: predicted price from neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16664721-85f1-4eb6-b6de-c9f27d86943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom transformer\n",
    "knn_price_feature = KNNPriceFeature(n_neighbors=5)\n",
    "\n",
    "# Add to existing pipeline\n",
    "preprocessing_with_knn = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                           \"households\", \"median_income\"]),\n",
    "    (\"geo_similarity\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "    (\"geo_knn_price\", knn_price_feature, [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\", cat_pipeline, [\"ocean_proximity\"])\n",
    "], remainder=default_num_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7a7dd-6941-4a25-853d-4fe898573535",
   "metadata": {},
   "source": [
    "### Step 1: **Set Up Feature Selector Using SelectFromModel**\n",
    "\n",
    "**Feature Selector**\n",
    "* We use `SelectFromModel` to keep only the most important features based on a Random Forest's feature importances. The threshold is set to \"median\", which keeps the top 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d41ecc2a-2cee-42fb-bac4-48305fae2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Feature selector using Random Forest\n",
    "feature_selector = SelectFromModel(\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    threshold=\"median\"  # Keep top 50% of features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5eecf4-2d98-495a-8328-39739288539c",
   "metadata": {},
   "source": [
    "### Step 2: **Build Full Pipeline with Feature Selection**\n",
    "**Full Pipeline**\n",
    "* We create a unified pipeline that handles preprocessing using knn, feature selection, and modeling in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e9f208-d549-4a0f-ad38-7b2eaf50e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Full pipeline: preprocessing_with_knn → feature selection → SVR\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_with_knn),\n",
    "    (\"feature_selection\", feature_selector),\n",
    "    (\"svr\", SVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856708e6-75e1-4a06-93dd-286c937c6748",
   "metadata": {},
   "source": [
    "### Step 3: **Run RandomizedSearchCV on the New Pipeline**\n",
    "**Train with RandomizedSearchCV**\n",
    "* We search over hyperparameters and train the entire pipeline using 3-fold CV. All steps (including feature selection) are executed inside the CV folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc2a2b-299a-4c83-a697-02beca936bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_distributions = {\n",
    "    \"svr__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"svr__C\": loguniform(0.01, 100),\n",
    "    \"svr__gamma\": loguniform(0.001, 1)  # used only if kernel is 'rbf'\n",
    "}\n",
    "\n",
    "# Subset data to 5,000 samples\n",
    "X_small = housing[:5000]\n",
    "y_small = housing_labels[:5000]\n",
    "\n",
    "# Run search\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_small, y_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc59ac-6cf2-4789-937f-cb43c28fdcb9",
   "metadata": {},
   "source": [
    "### Step 4: **Evaluate Best Model on Test Set**\n",
    "**Test Evaluation**\n",
    "* We evaluate the final pipeline (including automatic feature selection) on the full test set and compute RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e69ad-32f3-4b56-9526-a2de1723557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare raw test data\n",
    "X_test = strat_test_set.drop([\"median_house_value\", \"income_cat\"], axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Predict with best pipeline\n",
    "final_model = random_search.best_estimator_\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Compute RMSE manually\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Final Test Set RMSE:\", test_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

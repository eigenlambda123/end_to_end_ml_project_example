{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336fcdca-198f-494a-bf11-8cf8bdcaf605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")  # Path to the compressed dataset\n",
    "\n",
    "    if not tarball_path.is_file():  # If the file doesn't exist locally\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)  # Create the 'datasets' directory if needed\n",
    "\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"  # URL to download the dataset\n",
    "        urllib.request.urlretrieve(url, tarball_path)  # Download the .tgz file from the URL and save it locally\n",
    "\n",
    "        with tarfile.open(tarball_path) as housing_tarball:  # Open the .tgz file as a tar archive\n",
    "            housing_tarball.extractall(path=\"datasets\")  # Extract all contents into the 'datasets' directory\n",
    "\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))  # Load the CSV data into a DataFrame and return it\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245b667a-42bd-453d-93b8-3530f674ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"] = pd.cut(\n",
    "    housing[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing,\n",
    "    test_size=0.2,\n",
    "    stratify=housing[\"income_cat\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dfe2262-b1a4-407b-a688-cfac4e15443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\") # set imputer\n",
    "housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dc5ec-0454-4c3e-91a0-d23797b34648",
   "metadata": {},
   "source": [
    "### Feature Scaling and Transformation\n",
    "\n",
    "* One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don't perform well when the input numerical attributes have very different scales.\n",
    "\n",
    "* This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\n",
    "\n",
    "* There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "As with all estimators, it is important to fit the scalers to the training data only: never use `fit()` or `fit_transform()` for anything else than the training set. Once you have a trained scaler, you can then use it to `transform()` any other set, including the validation set, the test set, and new data. Note that while the training set values will always be scaled to the specified range, if new data contains outliers, these may end up scaled outside the range. If you want to avoid this, just set the `clip` hyperparameter to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27f143-8131-4f6c-9cce-984b6794c107",
   "metadata": {},
   "source": [
    "---\n",
    "### Min-max Scaling (Normalization)\n",
    "\n",
    "* Min-max scaling (many people call this normalization) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value and dividing by the difference between the min and the max. \n",
    "\n",
    "* Scikit-Learn provides a transformer called `MinMaxScaler` for this. It has a `feature_range` hyperparameter that lets you change the range if, for some reason, you don't want 0-1 (e.g., neural networks work best with zero-mean inputs, so a range of -1 to 1 is preferable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cbc1d5-0c85-423e-8e96-6c74151a19c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60851927,  0.11702128,  1.        , ..., -0.61433638,\n",
       "        -0.7794789 ,  0.82803782],\n",
       "       [ 0.21095335, -0.66170213,  0.52941176, ..., -0.86708979,\n",
       "        -0.22929339,  0.93319203],\n",
       "       [-0.51926978,  0.23617021,  0.25490196, ..., -0.92458466,\n",
       "        -0.73336919, -0.64247158],\n",
       "       ...,\n",
       "       [ 0.47870183, -0.99148936, -0.52941176, ..., -0.71663244,\n",
       "        -0.50873781, -0.44824557],\n",
       "       [ 0.20689655, -0.6787234 ,  0.41176471, ..., -0.68751167,\n",
       "        -0.49716556,  1.        ],\n",
       "       [-0.60649087,  0.08723404,  0.68627451, ..., -0.92122457,\n",
       "        -0.61608805, -0.0997934 ]], shape=(16512, 9))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n",
    "housing_num_min_max_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7a5d1-430d-4f44-a7f7-e67c86849799",
   "metadata": {},
   "source": [
    "---\n",
    "### Standardization\n",
    "\n",
    "* Standardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). \n",
    "\n",
    "* Unlike min-max scaling, standardization does not restrict values to a specific range. However, standardization is much less affected by outliers. For example, suppose a district has a median income equal to 100 (by mistake), instead of the usual 0-15. Min-max scaling to the 0-1 range would map this outlier down to 1 and it would crush all the other values down to 0-0.15, whereas standardization would not be much affected. \n",
    "\n",
    "* Scikit-Learn provides a transformer called `StandardScaler` for standardization.\n",
    "\n",
    "**TIP**\n",
    "\n",
    "If you want to scale a sparse matrix without converting it to a dense matrix first, you can use a `StandardScaler` with its `with_mean` hyperparameter set to `False`: it will only divide the data by the standard deviation, without subtracting the mean (as this would break sparsity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3941da7c-366c-45be-96be-60e049e2fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.42303652,  1.0136059 ,  1.86111875, ...,  1.39481249,\n",
       "        -0.93649149,  2.18511202],\n",
       "       [ 0.59639445, -0.702103  ,  0.90762971, ..., -0.37348471,\n",
       "         1.17194198,  2.40625396],\n",
       "       [-1.2030985 ,  1.27611874,  0.35142777, ..., -0.77572662,\n",
       "        -0.75978881, -0.90740625],\n",
       "       ...,\n",
       "       [ 1.25620853, -1.42870103, -1.23772062, ...,  0.67913534,\n",
       "         0.1010487 , -0.49894408],\n",
       "       [ 0.58639727, -0.73960483,  0.66925745, ...,  0.88286825,\n",
       "         0.14539615,  2.54675281],\n",
       "       [-1.41803793,  0.94797769,  1.22545939, ..., -0.75221898,\n",
       "        -0.31034135,  0.23385961]], shape=(16512, 9))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)\n",
    "housing_num_std_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
